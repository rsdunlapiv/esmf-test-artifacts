2022-07-14 02:23:12
20220714 031305.943 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET0 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET0 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET0 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET0 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET0 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET0 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET0 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET0 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET0 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET0 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET0 esmfComm=mpt
20220714 031305.943 INFO             PET0 isPthreadsEnabled=1
20220714 031305.943 INFO             PET0 isOpenMPEnabled=1
20220714 031305.943 INFO             PET0 isOpenACCEnabled=0
20220714 031305.944 INFO             PET0 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET0 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET0 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET0 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET0 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET0 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET0 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET0 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET0 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET0 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET0 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET0 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET0 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.946 INFO             PET0 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.946 INFO             PET0 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.946 INFO             PET0 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.946 INFO             PET0 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.946 INFO             PET0 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.946 INFO             PET0 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.946 INFO             PET0 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.946 INFO             PET0 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.946 INFO             PET0 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.946 INFO             PET0 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.946 INFO             PET0 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.946 INFO             PET0 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.946 INFO             PET0 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.946 INFO             PET0 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.946 INFO             PET0 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.946 INFO             PET0 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.946 INFO             PET0 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.946 INFO             PET0 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.946 INFO             PET0 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.946 INFO             PET0 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.946 INFO             PET0 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.946 INFO             PET0 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.946 INFO             PET0 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.946 INFO             PET0 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.946 INFO             PET0 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.946 INFO             PET0 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.946 INFO             PET0 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.946 INFO             PET0 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.946 INFO             PET0 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.946 INFO             PET0 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.946 INFO             PET0 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.946 INFO             PET0 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.946 INFO             PET0 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.946 INFO             PET0 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.946 INFO             PET0 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.946 INFO             PET0 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.946 INFO             PET0 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.946 INFO             PET0 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.946 INFO             PET0 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.947 INFO             PET0 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.947 INFO             PET0 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.947 INFO             PET0 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET0 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.947 INFO             PET0 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.947 INFO             PET0 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.947 INFO             PET0 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.947 INFO             PET0 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.947 INFO             PET0 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.947 INFO             PET0 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.947 INFO             PET0 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.947 INFO             PET0 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.947 INFO             PET0 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.947 INFO             PET0 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.947 INFO             PET0 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.947 INFO             PET0 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.947 INFO             PET0 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.947 INFO             PET0 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.947 INFO             PET0 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.947 INFO             PET0 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.947 INFO             PET0 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.947 INFO             PET0 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.947 INFO             PET0 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.947 INFO             PET0 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.947 INFO             PET0 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.947 INFO             PET0 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.947 INFO             PET0 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.947 INFO             PET0 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.947 INFO             PET0 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.947 INFO             PET0 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.947 INFO             PET0 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.947 INFO             PET0 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET0 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.947 INFO             PET0 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.947 INFO             PET0 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.947 INFO             PET0 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.947 INFO             PET0 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.947 INFO             PET0 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.947 INFO             PET0 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.947 INFO             PET0 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.947 INFO             PET0 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.947 INFO             PET0 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.947 INFO             PET0 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.947 INFO             PET0 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.947 INFO             PET0 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.947 INFO             PET0 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET0 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET0 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET0 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET0 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.947 INFO             PET0 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.947 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220714 031305.947 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220714 031305.947 INFO             PET0 main: vm located at: 0x24c78d0
20220714 031305.947 INFO             PET0 main: petCount=6 localPet=0 mypthid=47161411839488 currentSsiPe=0
20220714 031305.947 INFO             PET0 main: Current system level affinity pinning for local PET:
20220714 031305.947 INFO             PET0 main:  SSIPE=0
20220714 031305.947 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.947 INFO             PET0 main: ssiCount=1 localSsi=0
20220714 031305.947 INFO             PET0 main: mpionly=1 threadsflag=0
20220714 031305.947 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.948 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.948 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.948 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.948 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.948 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.948 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.948 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.948 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET0 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET0 Executing 'userm1_register'
20220714 031305.949 INFO             PET0 Executing 'userm2_setvm'
20220714 031305.950 INFO             PET0 Executing 'userm2_register'
20220714 031305.951 INFO             PET0 Entering 'user1_run'
20220714 031305.951 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET0 model1: vm located at: 0x354af80
20220714 031305.951 INFO             PET0 model1: petCount=6 localPet=0 mypthid=47161411839488 currentSsiPe=0
20220714 031305.952 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET0 model1:  SSIPE=0
20220714 031305.952 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET0 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET0 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220714 031305.943 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET1 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET1 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET1 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET1 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET1 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET1 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET1 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET1 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET1 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET1 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET1 esmfComm=mpt
20220714 031305.943 INFO             PET1 isPthreadsEnabled=1
20220714 031305.943 INFO             PET1 isOpenMPEnabled=1
20220714 031305.943 INFO             PET1 isOpenACCEnabled=0
20220714 031305.944 INFO             PET1 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET1 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET1 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET1 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET1 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET1 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET1 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET1 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET1 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET1 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET1 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET1 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET1 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.946 INFO             PET1 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.946 INFO             PET1 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.946 INFO             PET1 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.946 INFO             PET1 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.946 INFO             PET1 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.946 INFO             PET1 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.946 INFO             PET1 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.946 INFO             PET1 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.946 INFO             PET1 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.946 INFO             PET1 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.946 INFO             PET1 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.946 INFO             PET1 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.946 INFO             PET1 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.946 INFO             PET1 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.946 INFO             PET1 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.946 INFO             PET1 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.946 INFO             PET1 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.946 INFO             PET1 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.946 INFO             PET1 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.946 INFO             PET1 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.946 INFO             PET1 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.946 INFO             PET1 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.946 INFO             PET1 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.946 INFO             PET1 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.946 INFO             PET1 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.946 INFO             PET1 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.946 INFO             PET1 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.946 INFO             PET1 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.946 INFO             PET1 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.946 INFO             PET1 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.946 INFO             PET1 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.946 INFO             PET1 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.946 INFO             PET1 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.946 INFO             PET1 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.946 INFO             PET1 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.946 INFO             PET1 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.946 INFO             PET1 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.947 INFO             PET1 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.947 INFO             PET1 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.947 INFO             PET1 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.947 INFO             PET1 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.947 INFO             PET1 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET1 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.947 INFO             PET1 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.947 INFO             PET1 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.947 INFO             PET1 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.947 INFO             PET1 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.947 INFO             PET1 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.947 INFO             PET1 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.947 INFO             PET1 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.947 INFO             PET1 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.947 INFO             PET1 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.947 INFO             PET1 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.947 INFO             PET1 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.947 INFO             PET1 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.947 INFO             PET1 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.947 INFO             PET1 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.947 INFO             PET1 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.947 INFO             PET1 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.947 INFO             PET1 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.947 INFO             PET1 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.947 INFO             PET1 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.947 INFO             PET1 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.947 INFO             PET1 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.947 INFO             PET1 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.947 INFO             PET1 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.947 INFO             PET1 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.947 INFO             PET1 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.947 INFO             PET1 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.947 INFO             PET1 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.947 INFO             PET1 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET1 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.947 INFO             PET1 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.947 INFO             PET1 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.947 INFO             PET1 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.947 INFO             PET1 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.947 INFO             PET1 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.947 INFO             PET1 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.947 INFO             PET1 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.947 INFO             PET1 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.947 INFO             PET1 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.947 INFO             PET1 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.947 INFO             PET1 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.947 INFO             PET1 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.947 INFO             PET1 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET1 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET1 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET1 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET1 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.947 INFO             PET1 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.947 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220714 031305.947 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220714 031305.947 INFO             PET1 main: vm located at: 0x24c7980
20220714 031305.947 INFO             PET1 main: petCount=6 localPet=1 mypthid=47161411839488 currentSsiPe=1
20220714 031305.947 INFO             PET1 main: Current system level affinity pinning for local PET:
20220714 031305.947 INFO             PET1 main:  SSIPE=1
20220714 031305.948 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.948 INFO             PET1 main: ssiCount=1 localSsi=0
20220714 031305.948 INFO             PET1 main: mpionly=1 threadsflag=0
20220714 031305.948 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.948 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.948 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.948 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.948 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.948 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.948 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.948 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.948 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET1 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET1 Executing 'userm1_register'
20220714 031305.949 INFO             PET1 Executing 'userm2_setvm'
20220714 031305.949 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.949 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.951 INFO             PET1 Entering 'user1_run'
20220714 031305.951 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET1 model1: vm located at: 0x3576030
20220714 031305.951 INFO             PET1 model1: petCount=6 localPet=1 mypthid=47161411839488 currentSsiPe=1
20220714 031305.952 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET1 model1:  SSIPE=1
20220714 031305.952 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET1 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET1 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220714 031305.942 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET2 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET2 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET2 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET2 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET2 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET2 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET2 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET2 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET2 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET2 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET2 esmfComm=mpt
20220714 031305.943 INFO             PET2 isPthreadsEnabled=1
20220714 031305.943 INFO             PET2 isOpenMPEnabled=1
20220714 031305.943 INFO             PET2 isOpenACCEnabled=0
20220714 031305.944 INFO             PET2 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET2 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET2 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET2 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET2 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET2 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET2 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET2 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET2 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET2 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET2 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET2 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET2 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.947 INFO             PET2 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.947 INFO             PET2 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.947 INFO             PET2 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.947 INFO             PET2 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.947 INFO             PET2 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.947 INFO             PET2 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.947 INFO             PET2 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.947 INFO             PET2 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.947 INFO             PET2 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.947 INFO             PET2 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.947 INFO             PET2 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.947 INFO             PET2 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.947 INFO             PET2 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.947 INFO             PET2 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.947 INFO             PET2 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.947 INFO             PET2 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.947 INFO             PET2 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.947 INFO             PET2 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.947 INFO             PET2 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.947 INFO             PET2 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.947 INFO             PET2 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.947 INFO             PET2 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.947 INFO             PET2 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.947 INFO             PET2 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.947 INFO             PET2 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.947 INFO             PET2 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.947 INFO             PET2 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.947 INFO             PET2 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.947 INFO             PET2 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.947 INFO             PET2 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.947 INFO             PET2 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.947 INFO             PET2 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.947 INFO             PET2 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.947 INFO             PET2 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.947 INFO             PET2 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.947 INFO             PET2 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.947 INFO             PET2 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.947 INFO             PET2 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.947 INFO             PET2 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.947 INFO             PET2 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.947 INFO             PET2 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.947 INFO             PET2 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET2 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.947 INFO             PET2 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.947 INFO             PET2 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.947 INFO             PET2 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.947 INFO             PET2 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.947 INFO             PET2 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.947 INFO             PET2 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.947 INFO             PET2 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.947 INFO             PET2 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.947 INFO             PET2 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.947 INFO             PET2 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.947 INFO             PET2 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.947 INFO             PET2 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.947 INFO             PET2 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.947 INFO             PET2 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.947 INFO             PET2 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.947 INFO             PET2 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.947 INFO             PET2 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.947 INFO             PET2 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.947 INFO             PET2 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.947 INFO             PET2 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.947 INFO             PET2 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.947 INFO             PET2 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.948 INFO             PET2 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.948 INFO             PET2 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.948 INFO             PET2 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.948 INFO             PET2 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.948 INFO             PET2 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.948 INFO             PET2 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.948 INFO             PET2 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.948 INFO             PET2 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.948 INFO             PET2 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.948 INFO             PET2 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.948 INFO             PET2 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.948 INFO             PET2 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.948 INFO             PET2 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.948 INFO             PET2 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.948 INFO             PET2 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.948 INFO             PET2 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.948 INFO             PET2 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.948 INFO             PET2 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.948 INFO             PET2 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.948 INFO             PET2 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.948 INFO             PET2 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.948 INFO             PET2 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.948 INFO             PET2 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.948 INFO             PET2 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.948 INFO             PET2 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.948 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220714 031305.948 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220714 031305.948 INFO             PET2 main: vm located at: 0x24c7a20
20220714 031305.948 INFO             PET2 main: petCount=6 localPet=2 mypthid=47161411839488 currentSsiPe=2
20220714 031305.948 INFO             PET2 main: Current system level affinity pinning for local PET:
20220714 031305.948 INFO             PET2 main:  SSIPE=2
20220714 031305.948 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.948 INFO             PET2 main: ssiCount=1 localSsi=0
20220714 031305.948 INFO             PET2 main: mpionly=1 threadsflag=0
20220714 031305.948 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.948 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.948 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.948 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.948 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.948 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.948 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.948 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.948 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET2 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET2 Executing 'userm1_register'
20220714 031305.949 INFO             PET2 Executing 'userm2_setvm'
20220714 031305.949 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.949 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.951 INFO             PET2 Entering 'user1_run'
20220714 031305.951 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET2 model1: vm located at: 0x354bf60
20220714 031305.951 INFO             PET2 model1: petCount=6 localPet=2 mypthid=47161411839488 currentSsiPe=2
20220714 031305.951 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET2 model1:  SSIPE=2
20220714 031305.952 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET2 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET2 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220714 031305.942 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET3 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET3 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET3 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET3 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET3 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET3 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET3 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET3 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET3 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET3 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET3 esmfComm=mpt
20220714 031305.943 INFO             PET3 isPthreadsEnabled=1
20220714 031305.943 INFO             PET3 isOpenMPEnabled=1
20220714 031305.943 INFO             PET3 isOpenACCEnabled=0
20220714 031305.944 INFO             PET3 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET3 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET3 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET3 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET3 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET3 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET3 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET3 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET3 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET3 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET3 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET3 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET3 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.945 INFO             PET3 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.945 INFO             PET3 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.945 INFO             PET3 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.945 INFO             PET3 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.946 INFO             PET3 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.946 INFO             PET3 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.946 INFO             PET3 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.946 INFO             PET3 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.946 INFO             PET3 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.946 INFO             PET3 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.946 INFO             PET3 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.946 INFO             PET3 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.946 INFO             PET3 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.946 INFO             PET3 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.946 INFO             PET3 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.946 INFO             PET3 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.946 INFO             PET3 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.946 INFO             PET3 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.946 INFO             PET3 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.946 INFO             PET3 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.946 INFO             PET3 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.946 INFO             PET3 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.946 INFO             PET3 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.946 INFO             PET3 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.946 INFO             PET3 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.946 INFO             PET3 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.946 INFO             PET3 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.946 INFO             PET3 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.946 INFO             PET3 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.946 INFO             PET3 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.946 INFO             PET3 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.946 INFO             PET3 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.946 INFO             PET3 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.946 INFO             PET3 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.946 INFO             PET3 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.946 INFO             PET3 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.946 INFO             PET3 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.946 INFO             PET3 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.946 INFO             PET3 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.946 INFO             PET3 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.946 INFO             PET3 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.946 INFO             PET3 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.946 INFO             PET3 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.946 INFO             PET3 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.946 INFO             PET3 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.946 INFO             PET3 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.946 INFO             PET3 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.946 INFO             PET3 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.946 INFO             PET3 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.946 INFO             PET3 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.946 INFO             PET3 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.946 INFO             PET3 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.946 INFO             PET3 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.946 INFO             PET3 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.946 INFO             PET3 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.946 INFO             PET3 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.946 INFO             PET3 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.946 INFO             PET3 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.946 INFO             PET3 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.946 INFO             PET3 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.946 INFO             PET3 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.946 INFO             PET3 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.946 INFO             PET3 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.946 INFO             PET3 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.946 INFO             PET3 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.947 INFO             PET3 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.947 INFO             PET3 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.947 INFO             PET3 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.947 INFO             PET3 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.947 INFO             PET3 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.947 INFO             PET3 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET3 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.947 INFO             PET3 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.947 INFO             PET3 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.947 INFO             PET3 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.947 INFO             PET3 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.947 INFO             PET3 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.947 INFO             PET3 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.947 INFO             PET3 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.947 INFO             PET3 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.947 INFO             PET3 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.947 INFO             PET3 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.947 INFO             PET3 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.947 INFO             PET3 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.947 INFO             PET3 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET3 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET3 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET3 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET3 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.947 INFO             PET3 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.947 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220714 031305.947 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220714 031305.947 INFO             PET3 main: vm located at: 0x24c7ab0
20220714 031305.947 INFO             PET3 main: petCount=6 localPet=3 mypthid=47161411839488 currentSsiPe=3
20220714 031305.947 INFO             PET3 main: Current system level affinity pinning for local PET:
20220714 031305.947 INFO             PET3 main:  SSIPE=3
20220714 031305.947 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.947 INFO             PET3 main: ssiCount=1 localSsi=0
20220714 031305.947 INFO             PET3 main: mpionly=1 threadsflag=0
20220714 031305.947 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.947 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.947 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.947 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.947 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.947 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.947 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.947 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.947 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET3 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET3 Executing 'userm1_register'
20220714 031305.949 INFO             PET3 Executing 'userm2_setvm'
20220714 031305.950 INFO             PET3 Executing 'userm2_register'
20220714 031305.951 INFO             PET3 Entering 'user1_run'
20220714 031305.951 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET3 model1: vm located at: 0x354bfe0
20220714 031305.951 INFO             PET3 model1: petCount=6 localPet=3 mypthid=47161411839488 currentSsiPe=3
20220714 031305.951 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET3 model1:  SSIPE=3
20220714 031305.952 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET3 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET3 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220714 031305.942 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET4 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET4 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET4 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET4 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET4 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET4 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET4 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET4 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET4 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET4 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET4 esmfComm=mpt
20220714 031305.943 INFO             PET4 isPthreadsEnabled=1
20220714 031305.943 INFO             PET4 isOpenMPEnabled=1
20220714 031305.943 INFO             PET4 isOpenACCEnabled=0
20220714 031305.944 INFO             PET4 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET4 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET4 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET4 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET4 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET4 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET4 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET4 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET4 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET4 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET4 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET4 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET4 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.946 INFO             PET4 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.946 INFO             PET4 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.946 INFO             PET4 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.946 INFO             PET4 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.946 INFO             PET4 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.946 INFO             PET4 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.946 INFO             PET4 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.946 INFO             PET4 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.946 INFO             PET4 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.946 INFO             PET4 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.946 INFO             PET4 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.946 INFO             PET4 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.946 INFO             PET4 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.946 INFO             PET4 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.946 INFO             PET4 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.946 INFO             PET4 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.946 INFO             PET4 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.946 INFO             PET4 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.946 INFO             PET4 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.946 INFO             PET4 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.946 INFO             PET4 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.946 INFO             PET4 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.946 INFO             PET4 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.946 INFO             PET4 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.946 INFO             PET4 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.946 INFO             PET4 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.946 INFO             PET4 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.946 INFO             PET4 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.946 INFO             PET4 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.946 INFO             PET4 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.946 INFO             PET4 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.946 INFO             PET4 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.946 INFO             PET4 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.946 INFO             PET4 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.946 INFO             PET4 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.946 INFO             PET4 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.946 INFO             PET4 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.946 INFO             PET4 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.946 INFO             PET4 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.946 INFO             PET4 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.946 INFO             PET4 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.946 INFO             PET4 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.946 INFO             PET4 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.946 INFO             PET4 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.946 INFO             PET4 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.946 INFO             PET4 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.946 INFO             PET4 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.946 INFO             PET4 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.946 INFO             PET4 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.946 INFO             PET4 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.946 INFO             PET4 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.946 INFO             PET4 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.946 INFO             PET4 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.946 INFO             PET4 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.946 INFO             PET4 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.946 INFO             PET4 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.946 INFO             PET4 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.946 INFO             PET4 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.946 INFO             PET4 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.946 INFO             PET4 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.946 INFO             PET4 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.946 INFO             PET4 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.946 INFO             PET4 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.947 INFO             PET4 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.947 INFO             PET4 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.947 INFO             PET4 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.947 INFO             PET4 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.947 INFO             PET4 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.947 INFO             PET4 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.947 INFO             PET4 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.947 INFO             PET4 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.947 INFO             PET4 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.947 INFO             PET4 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.947 INFO             PET4 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.947 INFO             PET4 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.947 INFO             PET4 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.947 INFO             PET4 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.947 INFO             PET4 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.947 INFO             PET4 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.947 INFO             PET4 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.947 INFO             PET4 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.947 INFO             PET4 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.947 INFO             PET4 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.947 INFO             PET4 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.947 INFO             PET4 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET4 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET4 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET4 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET4 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.947 INFO             PET4 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.947 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220714 031305.947 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220714 031305.947 INFO             PET4 main: vm located at: 0x24c7b40
20220714 031305.947 INFO             PET4 main: petCount=6 localPet=4 mypthid=47161411839488 currentSsiPe=4
20220714 031305.947 INFO             PET4 main: Current system level affinity pinning for local PET:
20220714 031305.947 INFO             PET4 main:  SSIPE=4
20220714 031305.947 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.947 INFO             PET4 main: ssiCount=1 localSsi=0
20220714 031305.947 INFO             PET4 main: mpionly=1 threadsflag=0
20220714 031305.947 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.947 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.947 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.947 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.947 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.947 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.947 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.947 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.947 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET4 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET4 Executing 'userm1_register'
20220714 031305.949 INFO             PET4 Executing 'userm2_setvm'
20220714 031305.949 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.949 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.951 INFO             PET4 Entering 'user1_run'
20220714 031305.951 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET4 model1: vm located at: 0x354b1f0
20220714 031305.951 INFO             PET4 model1: petCount=6 localPet=4 mypthid=47161411839488 currentSsiPe=4
20220714 031305.951 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET4 model1:  SSIPE=4
20220714 031305.952 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET4 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET4 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220714 031305.943 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 031305.943 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 031305.943 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 031305.943 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 031305.943 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 031305.943 INFO             PET5 Running with ESMF Version   : v8.4.0b04
20220714 031305.943 INFO             PET5 ESMF library build date/time: "Jul 14 2022" "02:19:46"
20220714 031305.943 INFO             PET5 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/nvhpc_22.2_mpt_g_develop/esmf
20220714 031305.943 INFO             PET5 ESMF_COMM                   : mpt
20220714 031305.943 INFO             PET5 ESMF_MOAB                   : enabled
20220714 031305.943 INFO             PET5 ESMF_LAPACK                 : enabled
20220714 031305.943 INFO             PET5 ESMF_NETCDF                 : enabled
20220714 031305.943 INFO             PET5 ESMF_PNETCDF                : disabled
20220714 031305.943 INFO             PET5 ESMF_PIO                    : enabled
20220714 031305.943 INFO             PET5 ESMF_YAMLCPP                : enabled
20220714 031305.943 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220714 031305.943 INFO             PET5 esmfComm=mpt
20220714 031305.943 INFO             PET5 isPthreadsEnabled=1
20220714 031305.943 INFO             PET5 isOpenMPEnabled=1
20220714 031305.943 INFO             PET5 isOpenACCEnabled=0
20220714 031305.944 INFO             PET5 isSsiSharedMemoryEnabled=1
20220714 031305.944 INFO             PET5 ssiCount=1 peCount=6
20220714 031305.944 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220714 031305.944 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220714 031305.944 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220714 031305.944 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220714 031305.944 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220714 031305.944 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220714 031305.944 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220714 031305.944 INFO             PET5 index=   0                                         MPI_ABORT_TRACEBACK : Enables printing of the stack backtrace during MPI_Abort
20220714 031305.944 INFO             PET5 index=   1                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 031305.944 INFO             PET5 index=   2                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 031305.944 INFO             PET5 index=   3                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 031305.944 INFO             PET5 index=   4                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 031305.944 INFO             PET5 index=   5                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 031305.944 INFO             PET5 index=   6                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 031305.944 INFO             PET5 index=   7                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 031305.944 INFO             PET5 index=   8                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 031305.944 INFO             PET5 index=   9                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 031305.944 INFO             PET5 index=  10                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 031305.945 INFO             PET5 index=  11                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 031305.945 INFO             PET5 index=  12                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 031305.945 INFO             PET5 index=  13                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 031305.945 INFO             PET5 index=  14                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 031305.945 INFO             PET5 index=  15                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 031305.945 INFO             PET5 index=  16                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 031305.945 INFO             PET5 index=  17                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 031305.945 INFO             PET5 index=  18                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 031305.945 INFO             PET5 index=  19                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 031305.945 INFO             PET5 index=  20                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 031305.945 INFO             PET5 index=  21                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 031305.945 INFO             PET5 index=  22                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 031305.945 INFO             PET5 index=  23                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 031305.945 INFO             PET5 index=  24                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 031305.946 INFO             PET5 index=  25                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 031305.946 INFO             PET5 index=  26                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 031305.946 INFO             PET5 index=  27                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 031305.946 INFO             PET5 index=  28                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 031305.946 INFO             PET5 index=  29                                       MPI_COLL_MULTI_LEADER : If collectives optimization is enabled, controls whether MPT uses one node leader per CPU socket or per host to coalesce messages and improve performance
20220714 031305.946 INFO             PET5 index=  30                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 031305.946 INFO             PET5 index=  31                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 031305.946 INFO             PET5 index=  32                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 031305.946 INFO             PET5 index=  33                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 031305.946 INFO             PET5 index=  34                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 031305.946 INFO             PET5 index=  35                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 031305.946 INFO             PET5 index=  36                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 031305.946 INFO             PET5 index=  37                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 031305.946 INFO             PET5 index=  38                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 031305.946 INFO             PET5 index=  39                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 031305.946 INFO             PET5 index=  40                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 031305.946 INFO             PET5 index=  41                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 031305.946 INFO             PET5 index=  42                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 031305.946 INFO             PET5 index=  43                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 031305.946 INFO             PET5 index=  44                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 031305.946 INFO             PET5 index=  45                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 031305.946 INFO             PET5 index=  46                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 031305.946 INFO             PET5 index=  47                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 031305.946 INFO             PET5 index=  48                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 031305.946 INFO             PET5 index=  49                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 031305.946 INFO             PET5 index=  50                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 031305.946 INFO             PET5 index=  51                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 031305.946 INFO             PET5 index=  52                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 031305.946 INFO             PET5 index=  53                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 031305.946 INFO             PET5 index=  54                                         MPI_IP_ADDR_VERSION : If set, IP addresses of this type will be used for job launch and control
20220714 031305.946 INFO             PET5 index=  55                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 031305.946 INFO             PET5 index=  56                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 031305.946 INFO             PET5 index=  57                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 031305.946 INFO             PET5 index=  58                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 031305.946 INFO             PET5 index=  59                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 031305.946 INFO             PET5 index=  60                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 031305.946 INFO             PET5 index=  61                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 031305.946 INFO             PET5 index=  62                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 031305.946 INFO             PET5 index=  63                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 031305.946 INFO             PET5 index=  64                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 031305.946 INFO             PET5 index=  65                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 031305.946 INFO             PET5 index=  66                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 031305.946 INFO             PET5 index=  67                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 031305.946 INFO             PET5 index=  68                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 031305.946 INFO             PET5 index=  69                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 031305.946 INFO             PET5 index=  70                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 031305.946 INFO             PET5 index=  71                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 031305.946 INFO             PET5 index=  72                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 031305.946 INFO             PET5 index=  73                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 031305.946 INFO             PET5 index=  74                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 031305.946 INFO             PET5 index=  75                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 031305.946 INFO             PET5 index=  76                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 031305.946 INFO             PET5 index=  77                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 031305.946 INFO             PET5 index=  78                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 031305.946 INFO             PET5 index=  79                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 031305.946 INFO             PET5 index=  80                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 031305.946 INFO             PET5 index=  81                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 031305.946 INFO             PET5 index=  82                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 031305.946 INFO             PET5 index=  83                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 031305.946 INFO             PET5 index=  84                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 031305.946 INFO             PET5 index=  85                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 031305.946 INFO             PET5 index=  86                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 031305.947 INFO             PET5 index=  87                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 031305.947 INFO             PET5 index=  88                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 031305.947 INFO             PET5 index=  89                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 031305.947 INFO             PET5 index=  90                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 031305.947 INFO             PET5 index=  91                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 031305.947 INFO             PET5 index=  92                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 031305.947 INFO             PET5 index=  93                                               MPI_WORLD_MAP : Rank to host mapping
20220714 031305.947 INFO             PET5 index=  94                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 031305.947 INFO             PET5 index=  95                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET5 index=  96                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 031305.947 INFO             PET5 index=  97                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET5 index=  98                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 031305.947 INFO             PET5 index=  99                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 031305.947 INFO             PET5 index= 100                                    profiled_recv_request_id : identity of the request-of-interest
20220714 031305.947 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220714 031305.947 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220714 031305.947 INFO             PET5 main: vm located at: 0x24c7c00
20220714 031305.947 INFO             PET5 main: petCount=6 localPet=5 mypthid=47161411839488 currentSsiPe=5
20220714 031305.947 INFO             PET5 main: Current system level affinity pinning for local PET:
20220714 031305.947 INFO             PET5 main:  SSIPE=5
20220714 031305.947 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.947 INFO             PET5 main: ssiCount=1 localSsi=0
20220714 031305.947 INFO             PET5 main: mpionly=1 threadsflag=0
20220714 031305.947 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.947 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220714 031305.947 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220714 031305.947 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220714 031305.947 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220714 031305.947 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220714 031305.947 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.947 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220714 031305.947 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220714 031305.949 INFO             PET5 Executing 'userm1_setvm'
20220714 031305.949 INFO             PET5 Executing 'userm1_register'
20220714 031305.949 INFO             PET5 Executing 'userm2_setvm'
20220714 031305.949 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.949 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 031305.951 INFO             PET5 Entering 'user1_run'
20220714 031305.951 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220714 031305.951 INFO             PET5 model1: vm located at: 0x35762b0
20220714 031305.951 INFO             PET5 model1: petCount=6 localPet=5 mypthid=47161411839488 currentSsiPe=5
20220714 031305.952 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220714 031305.952 INFO             PET5 model1:  SSIPE=5
20220714 031305.952 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 031305.952 INFO             PET5 model1: ssiCount=1 localSsi=0
20220714 031305.952 INFO             PET5 model1: mpionly=1 threadsflag=0
20220714 031305.952 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 031305.952 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220714 031305.952 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220714 031305.952 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220714 031305.952 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220714 031305.952 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220714 031305.952 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 031305.952 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220714 031305.952 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
