2022-07-14 01:56:15
20220714 022036.017 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET0 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET0 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET0 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET0 ESMF_COMM                   : mpt
20220714 022036.018 INFO             PET0 ESMF_MOAB                   : enabled
20220714 022036.018 INFO             PET0 ESMF_LAPACK                 : enabled
20220714 022036.018 INFO             PET0 ESMF_NETCDF                 : enabled
20220714 022036.018 INFO             PET0 ESMF_PNETCDF                : disabled
20220714 022036.018 INFO             PET0 ESMF_PIO                    : enabled
20220714 022036.018 INFO             PET0 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET0 esmfComm=mpt
20220714 022036.018 INFO             PET0 isPthreadsEnabled=1
20220714 022036.018 INFO             PET0 isOpenMPEnabled=1
20220714 022036.018 INFO             PET0 isOpenACCEnabled=0
20220714 022036.018 INFO             PET0 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET0 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET0 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET0 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET0 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET0 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET0 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET0 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET0 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET0 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET0 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET0 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET0 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET0 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.020 INFO             PET0 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.020 INFO             PET0 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.020 INFO             PET0 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.020 INFO             PET0 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.020 INFO             PET0 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.020 INFO             PET0 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.020 INFO             PET0 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.020 INFO             PET0 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.020 INFO             PET0 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.020 INFO             PET0 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.020 INFO             PET0 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.020 INFO             PET0 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.020 INFO             PET0 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.020 INFO             PET0 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.020 INFO             PET0 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.020 INFO             PET0 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.020 INFO             PET0 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.020 INFO             PET0 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.020 INFO             PET0 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.020 INFO             PET0 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.020 INFO             PET0 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.020 INFO             PET0 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.020 INFO             PET0 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.020 INFO             PET0 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.020 INFO             PET0 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET0 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET0 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET0 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET0 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET0 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET0 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET0 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET0 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET0 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET0 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET0 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET0 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET0 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET0 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET0 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET0 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET0 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET0 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET0 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.021 INFO             PET0 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.021 INFO             PET0 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.021 INFO             PET0 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.021 INFO             PET0 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.021 INFO             PET0 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.021 INFO             PET0 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.021 INFO             PET0 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.021 INFO             PET0 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.021 INFO             PET0 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.021 INFO             PET0 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.021 INFO             PET0 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.021 INFO             PET0 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.021 INFO             PET0 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.021 INFO             PET0 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.021 INFO             PET0 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.021 INFO             PET0 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.021 INFO             PET0 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.021 INFO             PET0 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.021 INFO             PET0 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.021 INFO             PET0 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.021 INFO             PET0 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.021 INFO             PET0 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.021 INFO             PET0 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET0 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.021 INFO             PET0 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.021 INFO             PET0 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.021 INFO             PET0 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.021 INFO             PET0 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.021 INFO             PET0 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.021 INFO             PET0 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.021 INFO             PET0 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.021 INFO             PET0 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.021 INFO             PET0 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.021 INFO             PET0 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.021 INFO             PET0 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.021 INFO             PET0 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.021 INFO             PET0 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.021 INFO             PET0 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.021 INFO             PET0 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.021 INFO             PET0 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.021 INFO             PET0 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.021 INFO             PET0 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.021 INFO             PET0 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.021 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220714 022036.021 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220714 022036.021 INFO             PET0 main: vm located at: 0x1bddde0
20220714 022036.021 INFO             PET0 main: petCount=6 localPet=0 mypthid=47435323442432 currentSsiPe=0
20220714 022036.021 INFO             PET0 main: Current system level affinity pinning for local PET:
20220714 022036.021 INFO             PET0 main:  SSIPE=0
20220714 022036.021 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.021 INFO             PET0 main: ssiCount=1 localSsi=0
20220714 022036.021 INFO             PET0 main: mpionly=1 threadsflag=0
20220714 022036.021 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.021 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.021 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.021 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.021 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.021 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.021 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.021 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.021 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.021 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.021 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET0 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET0 Executing 'userm1_register'
20220714 022036.023 INFO             PET0 Executing 'userm2_setvm'
20220714 022036.024 INFO             PET0 Executing 'userm2_register'
20220714 022036.025 INFO             PET0 Entering 'user1_run'
20220714 022036.025 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET0 model1: vm located at: 0x2c62970
20220714 022036.025 INFO             PET0 model1: petCount=6 localPet=0 mypthid=47435323442432 currentSsiPe=0
20220714 022036.025 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET0 model1:  SSIPE=0
20220714 022036.025 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET0 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET0 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.025 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022037.494 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022038.881 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022040.267 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022041.653 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022043.038 INFO             PET0 Exiting 'user1_run'
20220714 022043.194 INFO             PET0 Entering 'user2_run'
20220714 022043.194 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220714 022043.194 INFO             PET0 model2: vm located at: 0x2c62660
20220714 022043.194 INFO             PET0 model2: petCount=2 localPet=0 mypthid=47435323442432 currentSsiPe=0
20220714 022043.194 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220714 022043.194 INFO             PET0 model2:  SSIPE=0
20220714 022043.194 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220714 022043.194 INFO             PET0 model2: ssiCount=1 localSsi=0
20220714 022043.194 INFO             PET0 model2: mpionly=1 threadsflag=0
20220714 022043.194 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022043.194 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220714 022043.194 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220714 022043.194 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220714 022043.194 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220714 022043.194 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220714 022043.194 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220714 022043.194 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220714 022043.194 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220714 022043.194 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220714 022043.195 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022043.195 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022043.195 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022044.592 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022044.592 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022044.592 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022045.985 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022045.985 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022045.985 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022047.377 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022047.377 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022047.377 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022048.770 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022048.770 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022048.770 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022050.163 INFO             PET0  user2_run: All data correct.
20220714 022050.163 INFO             PET0 Exiting 'user2_run'
20220714 022050.163 INFO             PET0 Entering 'user1_run'
20220714 022050.163 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220714 022050.163 INFO             PET0 model1: vm located at: 0x2c62970
20220714 022050.163 INFO             PET0 model1: petCount=6 localPet=0 mypthid=47435323442432 currentSsiPe=0
20220714 022050.163 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220714 022050.163 INFO             PET0 model1:  SSIPE=0
20220714 022050.163 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.163 INFO             PET0 model1: ssiCount=1 localSsi=0
20220714 022050.163 INFO             PET0 model1: mpionly=1 threadsflag=0
20220714 022050.163 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.163 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.163 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.163 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.163 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.163 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.163 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.163 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.163 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220714 022050.163 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022051.549 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022052.935 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022054.322 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022055.708 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220714 022057.094 INFO             PET0 Exiting 'user1_run'
20220714 022057.200 INFO             PET0 Entering 'user2_run'
20220714 022057.200 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220714 022057.200 INFO             PET0 model2: vm located at: 0x2c62660
20220714 022057.200 INFO             PET0 model2: petCount=2 localPet=0 mypthid=47435323442432 currentSsiPe=0
20220714 022057.200 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220714 022057.200 INFO             PET0 model2:  SSIPE=0
20220714 022057.200 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220714 022057.200 INFO             PET0 model2: ssiCount=1 localSsi=0
20220714 022057.200 INFO             PET0 model2: mpionly=1 threadsflag=0
20220714 022057.200 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022057.200 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220714 022057.200 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220714 022057.200 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220714 022057.200 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220714 022057.200 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220714 022057.200 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220714 022057.200 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220714 022057.200 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220714 022057.200 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220714 022057.200 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022057.200 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022057.200 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022058.593 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022058.593 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022058.593 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022059.985 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022059.985 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022059.985 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022101.378 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022101.378 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022101.378 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022102.770 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220714 022102.770 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022102.770 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022104.163 INFO             PET0  user2_run: All data correct.
20220714 022104.163 INFO             PET0 Exiting 'user2_run'
20220714 022104.163 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET0  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET0 Finalizing ESMF
20220714 022036.017 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET1 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET1 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET1 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET1 ESMF_COMM                   : mpt
20220714 022036.017 INFO             PET1 ESMF_MOAB                   : enabled
20220714 022036.017 INFO             PET1 ESMF_LAPACK                 : enabled
20220714 022036.017 INFO             PET1 ESMF_NETCDF                 : enabled
20220714 022036.017 INFO             PET1 ESMF_PNETCDF                : disabled
20220714 022036.017 INFO             PET1 ESMF_PIO                    : enabled
20220714 022036.017 INFO             PET1 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET1 esmfComm=mpt
20220714 022036.018 INFO             PET1 isPthreadsEnabled=1
20220714 022036.018 INFO             PET1 isOpenMPEnabled=1
20220714 022036.018 INFO             PET1 isOpenACCEnabled=0
20220714 022036.018 INFO             PET1 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET1 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET1 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET1 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET1 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET1 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET1 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET1 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET1 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET1 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET1 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET1 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET1 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET1 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.021 INFO             PET1 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.021 INFO             PET1 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.021 INFO             PET1 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.021 INFO             PET1 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.021 INFO             PET1 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.021 INFO             PET1 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.021 INFO             PET1 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.021 INFO             PET1 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.021 INFO             PET1 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.021 INFO             PET1 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.021 INFO             PET1 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.021 INFO             PET1 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.021 INFO             PET1 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.021 INFO             PET1 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.021 INFO             PET1 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.021 INFO             PET1 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.021 INFO             PET1 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.021 INFO             PET1 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.021 INFO             PET1 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.021 INFO             PET1 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.021 INFO             PET1 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.021 INFO             PET1 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.021 INFO             PET1 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.021 INFO             PET1 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.021 INFO             PET1 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET1 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET1 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET1 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET1 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET1 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET1 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET1 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET1 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET1 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET1 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET1 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET1 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET1 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET1 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET1 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET1 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET1 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET1 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET1 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.021 INFO             PET1 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.022 INFO             PET1 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.022 INFO             PET1 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.022 INFO             PET1 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.022 INFO             PET1 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.022 INFO             PET1 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.022 INFO             PET1 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.022 INFO             PET1 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.022 INFO             PET1 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.022 INFO             PET1 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.022 INFO             PET1 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.022 INFO             PET1 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.022 INFO             PET1 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.022 INFO             PET1 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.022 INFO             PET1 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.022 INFO             PET1 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.022 INFO             PET1 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.022 INFO             PET1 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.022 INFO             PET1 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.022 INFO             PET1 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.022 INFO             PET1 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.022 INFO             PET1 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.022 INFO             PET1 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.022 INFO             PET1 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.022 INFO             PET1 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.022 INFO             PET1 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.022 INFO             PET1 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.022 INFO             PET1 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.022 INFO             PET1 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.022 INFO             PET1 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.022 INFO             PET1 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.022 INFO             PET1 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.022 INFO             PET1 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.022 INFO             PET1 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.022 INFO             PET1 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.022 INFO             PET1 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.022 INFO             PET1 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET1 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET1 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET1 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET1 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.022 INFO             PET1 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.022 INFO             PET1 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.022 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220714 022036.022 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220714 022036.022 INFO             PET1 main: vm located at: 0x1bddfe0
20220714 022036.022 INFO             PET1 main: petCount=6 localPet=1 mypthid=47435323442432 currentSsiPe=1
20220714 022036.022 INFO             PET1 main: Current system level affinity pinning for local PET:
20220714 022036.022 INFO             PET1 main:  SSIPE=1
20220714 022036.022 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.022 INFO             PET1 main: ssiCount=1 localSsi=0
20220714 022036.022 INFO             PET1 main: mpionly=1 threadsflag=0
20220714 022036.022 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.022 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.022 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.022 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.022 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.022 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET1 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET1 Executing 'userm1_register'
20220714 022036.023 INFO             PET1 Executing 'userm2_setvm'
20220714 022036.023 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.023 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.025 INFO             PET1 Entering 'user1_run'
20220714 022036.025 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET1 model1: vm located at: 0x2c619f0
20220714 022036.025 INFO             PET1 model1: petCount=6 localPet=1 mypthid=47435323442432 currentSsiPe=1
20220714 022036.025 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET1 model1:  SSIPE=1
20220714 022036.025 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET1 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET1 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.025 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022037.507 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022038.907 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022040.307 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022041.707 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022043.107 INFO             PET1 Exiting 'user1_run'
20220714 022050.163 INFO             PET1 Entering 'user1_run'
20220714 022050.163 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220714 022050.163 INFO             PET1 model1: vm located at: 0x2c619f0
20220714 022050.163 INFO             PET1 model1: petCount=6 localPet=1 mypthid=47435323442432 currentSsiPe=1
20220714 022050.163 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220714 022050.163 INFO             PET1 model1:  SSIPE=1
20220714 022050.163 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.163 INFO             PET1 model1: ssiCount=1 localSsi=0
20220714 022050.163 INFO             PET1 model1: mpionly=1 threadsflag=0
20220714 022050.163 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.163 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.163 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.163 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.163 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.163 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.163 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.163 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.163 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220714 022050.163 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022051.563 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022052.963 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022054.363 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022055.763 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022057.162 INFO             PET1 Exiting 'user1_run'
20220714 022104.163 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET1  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET1 Finalizing ESMF
20220714 022036.017 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET2 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET2 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET2 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET2 ESMF_COMM                   : mpt
20220714 022036.017 INFO             PET2 ESMF_MOAB                   : enabled
20220714 022036.017 INFO             PET2 ESMF_LAPACK                 : enabled
20220714 022036.017 INFO             PET2 ESMF_NETCDF                 : enabled
20220714 022036.017 INFO             PET2 ESMF_PNETCDF                : disabled
20220714 022036.017 INFO             PET2 ESMF_PIO                    : enabled
20220714 022036.017 INFO             PET2 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET2 esmfComm=mpt
20220714 022036.018 INFO             PET2 isPthreadsEnabled=1
20220714 022036.018 INFO             PET2 isOpenMPEnabled=1
20220714 022036.018 INFO             PET2 isOpenACCEnabled=0
20220714 022036.018 INFO             PET2 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET2 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET2 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET2 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET2 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET2 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET2 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET2 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET2 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET2 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET2 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET2 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET2 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET2 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.021 INFO             PET2 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.021 INFO             PET2 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.021 INFO             PET2 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.021 INFO             PET2 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.021 INFO             PET2 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.021 INFO             PET2 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.021 INFO             PET2 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.021 INFO             PET2 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.021 INFO             PET2 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.021 INFO             PET2 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.021 INFO             PET2 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.021 INFO             PET2 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.021 INFO             PET2 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.021 INFO             PET2 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.021 INFO             PET2 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.021 INFO             PET2 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.021 INFO             PET2 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.021 INFO             PET2 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.021 INFO             PET2 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.021 INFO             PET2 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.021 INFO             PET2 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.021 INFO             PET2 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.021 INFO             PET2 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.021 INFO             PET2 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.021 INFO             PET2 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET2 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET2 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET2 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET2 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET2 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET2 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET2 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET2 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET2 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET2 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET2 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET2 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET2 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET2 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET2 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET2 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET2 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET2 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET2 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.021 INFO             PET2 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.021 INFO             PET2 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.021 INFO             PET2 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.021 INFO             PET2 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.021 INFO             PET2 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.021 INFO             PET2 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.021 INFO             PET2 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.021 INFO             PET2 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.021 INFO             PET2 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.021 INFO             PET2 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.021 INFO             PET2 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.021 INFO             PET2 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.021 INFO             PET2 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.021 INFO             PET2 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.021 INFO             PET2 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.021 INFO             PET2 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.021 INFO             PET2 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.021 INFO             PET2 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.021 INFO             PET2 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.021 INFO             PET2 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.021 INFO             PET2 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.021 INFO             PET2 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.021 INFO             PET2 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET2 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.021 INFO             PET2 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.021 INFO             PET2 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.021 INFO             PET2 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.021 INFO             PET2 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.021 INFO             PET2 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.021 INFO             PET2 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.021 INFO             PET2 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.021 INFO             PET2 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.021 INFO             PET2 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.021 INFO             PET2 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.021 INFO             PET2 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.021 INFO             PET2 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.021 INFO             PET2 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET2 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET2 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET2 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET2 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.022 INFO             PET2 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.022 INFO             PET2 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.022 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220714 022036.022 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220714 022036.022 INFO             PET2 main: vm located at: 0x1bddfe0
20220714 022036.022 INFO             PET2 main: petCount=6 localPet=2 mypthid=47435323442432 currentSsiPe=2
20220714 022036.022 INFO             PET2 main: Current system level affinity pinning for local PET:
20220714 022036.022 INFO             PET2 main:  SSIPE=2
20220714 022036.022 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.022 INFO             PET2 main: ssiCount=1 localSsi=0
20220714 022036.022 INFO             PET2 main: mpionly=1 threadsflag=0
20220714 022036.022 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.022 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.022 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.022 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.022 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.022 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET2 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET2 Executing 'userm1_register'
20220714 022036.023 INFO             PET2 Executing 'userm2_setvm'
20220714 022036.023 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.023 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.025 INFO             PET2 Entering 'user1_run'
20220714 022036.025 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET2 model1: vm located at: 0x2c61ae0
20220714 022036.025 INFO             PET2 model1: petCount=6 localPet=2 mypthid=47435323442432 currentSsiPe=2
20220714 022036.025 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET2 model1:  SSIPE=2
20220714 022036.025 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET2 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET2 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.026 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022037.514 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022038.921 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022040.329 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022041.736 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022043.144 INFO             PET2 Exiting 'user1_run'
20220714 022050.163 INFO             PET2 Entering 'user1_run'
20220714 022050.163 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220714 022050.163 INFO             PET2 model1: vm located at: 0x2c61ae0
20220714 022050.163 INFO             PET2 model1: petCount=6 localPet=2 mypthid=47435323442432 currentSsiPe=2
20220714 022050.163 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220714 022050.163 INFO             PET2 model1:  SSIPE=2
20220714 022050.163 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.163 INFO             PET2 model1: ssiCount=1 localSsi=0
20220714 022050.163 INFO             PET2 model1: mpionly=1 threadsflag=0
20220714 022050.163 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.163 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.163 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.163 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.163 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.163 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.163 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.163 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.163 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220714 022050.163 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022051.570 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022052.978 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022054.385 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022055.793 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220714 022057.200 INFO             PET2 Exiting 'user1_run'
20220714 022104.163 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET2  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET2 Finalizing ESMF
20220714 022036.017 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET3 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET3 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET3 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET3 ESMF_COMM                   : mpt
20220714 022036.017 INFO             PET3 ESMF_MOAB                   : enabled
20220714 022036.017 INFO             PET3 ESMF_LAPACK                 : enabled
20220714 022036.017 INFO             PET3 ESMF_NETCDF                 : enabled
20220714 022036.017 INFO             PET3 ESMF_PNETCDF                : disabled
20220714 022036.017 INFO             PET3 ESMF_PIO                    : enabled
20220714 022036.017 INFO             PET3 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET3 esmfComm=mpt
20220714 022036.018 INFO             PET3 isPthreadsEnabled=1
20220714 022036.018 INFO             PET3 isOpenMPEnabled=1
20220714 022036.018 INFO             PET3 isOpenACCEnabled=0
20220714 022036.018 INFO             PET3 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET3 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET3 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET3 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET3 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET3 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET3 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET3 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET3 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET3 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET3 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET3 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET3 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET3 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.021 INFO             PET3 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.021 INFO             PET3 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.021 INFO             PET3 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.021 INFO             PET3 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.021 INFO             PET3 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.021 INFO             PET3 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.021 INFO             PET3 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.021 INFO             PET3 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.021 INFO             PET3 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.021 INFO             PET3 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.021 INFO             PET3 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.021 INFO             PET3 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.021 INFO             PET3 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.021 INFO             PET3 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.021 INFO             PET3 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.021 INFO             PET3 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.021 INFO             PET3 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.021 INFO             PET3 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.021 INFO             PET3 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.021 INFO             PET3 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.021 INFO             PET3 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.021 INFO             PET3 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.021 INFO             PET3 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.021 INFO             PET3 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.021 INFO             PET3 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET3 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET3 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET3 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET3 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET3 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET3 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET3 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET3 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET3 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET3 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET3 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET3 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET3 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET3 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET3 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET3 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET3 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET3 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET3 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.021 INFO             PET3 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.021 INFO             PET3 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.021 INFO             PET3 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.021 INFO             PET3 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.021 INFO             PET3 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.021 INFO             PET3 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.021 INFO             PET3 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.021 INFO             PET3 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.021 INFO             PET3 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.021 INFO             PET3 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.021 INFO             PET3 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.021 INFO             PET3 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.021 INFO             PET3 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.021 INFO             PET3 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.021 INFO             PET3 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.021 INFO             PET3 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.021 INFO             PET3 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.021 INFO             PET3 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.021 INFO             PET3 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.021 INFO             PET3 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.021 INFO             PET3 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.021 INFO             PET3 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.021 INFO             PET3 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET3 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.021 INFO             PET3 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.022 INFO             PET3 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.022 INFO             PET3 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.022 INFO             PET3 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.022 INFO             PET3 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.022 INFO             PET3 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.022 INFO             PET3 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.022 INFO             PET3 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.022 INFO             PET3 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.022 INFO             PET3 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.022 INFO             PET3 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.022 INFO             PET3 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.022 INFO             PET3 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET3 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET3 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET3 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET3 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.022 INFO             PET3 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.022 INFO             PET3 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.022 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220714 022036.022 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220714 022036.022 INFO             PET3 main: vm located at: 0x1bde010
20220714 022036.022 INFO             PET3 main: petCount=6 localPet=3 mypthid=47435323442432 currentSsiPe=3
20220714 022036.022 INFO             PET3 main: Current system level affinity pinning for local PET:
20220714 022036.022 INFO             PET3 main:  SSIPE=3
20220714 022036.022 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.022 INFO             PET3 main: ssiCount=1 localSsi=0
20220714 022036.022 INFO             PET3 main: mpionly=1 threadsflag=0
20220714 022036.022 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.022 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.022 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.022 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.022 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.022 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET3 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET3 Executing 'userm1_register'
20220714 022036.023 INFO             PET3 Executing 'userm2_setvm'
20220714 022036.024 INFO             PET3 Executing 'userm2_register'
20220714 022036.025 INFO             PET3 Entering 'user1_run'
20220714 022036.025 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET3 model1: vm located at: 0x2c61ba0
20220714 022036.025 INFO             PET3 model1: petCount=6 localPet=3 mypthid=47435323442432 currentSsiPe=3
20220714 022036.025 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET3 model1:  SSIPE=3
20220714 022036.025 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET3 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET3 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.026 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022037.512 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022038.918 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022040.324 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022041.758 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022043.163 INFO             PET3 Exiting 'user1_run'
20220714 022043.194 INFO             PET3 Entering 'user2_run'
20220714 022043.194 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220714 022043.194 INFO             PET3 model2: vm located at: 0x2c62880
20220714 022043.194 INFO             PET3 model2: petCount=2 localPet=1 mypthid=47435323442432 currentSsiPe=3
20220714 022043.194 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220714 022043.194 INFO             PET3 model2:  SSIPE=3
20220714 022043.194 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220714 022043.194 INFO             PET3 model2: ssiCount=1 localSsi=0
20220714 022043.194 INFO             PET3 model2: mpionly=1 threadsflag=0
20220714 022043.194 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022043.194 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220714 022043.194 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220714 022043.194 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220714 022043.194 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220714 022043.194 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220714 022043.194 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220714 022043.194 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220714 022043.194 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220714 022043.194 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220714 022043.195 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022043.195 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022043.195 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022044.582 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022044.582 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022044.582 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022045.964 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022045.964 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022045.964 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022047.346 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022047.346 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022047.346 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022048.729 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022048.729 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022048.729 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022050.111 INFO             PET3  user2_run: All data correct.
20220714 022050.111 INFO             PET3 Exiting 'user2_run'
20220714 022050.111 INFO             PET3 Entering 'user1_run'
20220714 022050.111 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220714 022050.111 INFO             PET3 model1: vm located at: 0x2c61ba0
20220714 022050.111 INFO             PET3 model1: petCount=6 localPet=3 mypthid=47435323442432 currentSsiPe=3
20220714 022050.111 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220714 022050.111 INFO             PET3 model1:  SSIPE=3
20220714 022050.111 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.111 INFO             PET3 model1: ssiCount=1 localSsi=0
20220714 022050.111 INFO             PET3 model1: mpionly=1 threadsflag=0
20220714 022050.111 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.111 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.111 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.111 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.111 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.111 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.111 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.111 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.111 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220714 022050.111 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022051.517 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022052.922 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022054.327 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022055.732 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022057.137 INFO             PET3 Exiting 'user1_run'
20220714 022057.197 INFO             PET3 Entering 'user2_run'
20220714 022057.197 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220714 022057.197 INFO             PET3 model2: vm located at: 0x2c62880
20220714 022057.197 INFO             PET3 model2: petCount=2 localPet=1 mypthid=47435323442432 currentSsiPe=3
20220714 022057.197 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220714 022057.197 INFO             PET3 model2:  SSIPE=3
20220714 022057.197 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220714 022057.197 INFO             PET3 model2: ssiCount=1 localSsi=0
20220714 022057.197 INFO             PET3 model2: mpionly=1 threadsflag=0
20220714 022057.197 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022057.197 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220714 022057.197 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220714 022057.197 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220714 022057.197 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220714 022057.197 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220714 022057.197 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220714 022057.197 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220714 022057.197 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220714 022057.197 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220714 022057.197 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022057.197 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022057.197 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022058.585 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022058.585 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022058.585 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022059.967 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022059.968 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022059.968 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022101.350 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022101.350 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022101.350 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022102.732 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220714 022102.732 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220714 022102.732 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022104.120 INFO             PET3  user2_run: All data correct.
20220714 022104.120 INFO             PET3 Exiting 'user2_run'
20220714 022104.163 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET3  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET3 Finalizing ESMF
20220714 022036.017 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET4 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET4 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET4 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET4 ESMF_COMM                   : mpt
20220714 022036.017 INFO             PET4 ESMF_MOAB                   : enabled
20220714 022036.017 INFO             PET4 ESMF_LAPACK                 : enabled
20220714 022036.017 INFO             PET4 ESMF_NETCDF                 : enabled
20220714 022036.017 INFO             PET4 ESMF_PNETCDF                : disabled
20220714 022036.017 INFO             PET4 ESMF_PIO                    : enabled
20220714 022036.017 INFO             PET4 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET4 esmfComm=mpt
20220714 022036.018 INFO             PET4 isPthreadsEnabled=1
20220714 022036.018 INFO             PET4 isOpenMPEnabled=1
20220714 022036.018 INFO             PET4 isOpenACCEnabled=0
20220714 022036.018 INFO             PET4 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET4 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET4 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET4 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET4 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET4 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET4 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET4 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET4 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET4 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET4 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET4 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET4 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET4 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.021 INFO             PET4 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.021 INFO             PET4 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.021 INFO             PET4 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.021 INFO             PET4 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.021 INFO             PET4 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.021 INFO             PET4 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.021 INFO             PET4 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.021 INFO             PET4 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.021 INFO             PET4 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.021 INFO             PET4 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.021 INFO             PET4 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.021 INFO             PET4 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.021 INFO             PET4 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.021 INFO             PET4 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.021 INFO             PET4 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.021 INFO             PET4 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.021 INFO             PET4 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.021 INFO             PET4 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.021 INFO             PET4 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.021 INFO             PET4 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.021 INFO             PET4 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.021 INFO             PET4 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.021 INFO             PET4 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.021 INFO             PET4 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.021 INFO             PET4 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET4 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET4 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET4 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET4 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET4 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET4 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET4 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET4 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET4 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET4 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET4 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET4 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET4 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET4 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET4 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET4 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET4 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET4 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET4 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.022 INFO             PET4 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.022 INFO             PET4 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.022 INFO             PET4 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.022 INFO             PET4 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.022 INFO             PET4 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.022 INFO             PET4 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.022 INFO             PET4 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.022 INFO             PET4 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.022 INFO             PET4 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.022 INFO             PET4 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.022 INFO             PET4 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.022 INFO             PET4 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.022 INFO             PET4 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.022 INFO             PET4 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.022 INFO             PET4 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.022 INFO             PET4 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.022 INFO             PET4 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.022 INFO             PET4 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.022 INFO             PET4 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.022 INFO             PET4 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.022 INFO             PET4 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.022 INFO             PET4 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.022 INFO             PET4 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.022 INFO             PET4 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.022 INFO             PET4 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.022 INFO             PET4 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.022 INFO             PET4 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.022 INFO             PET4 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.022 INFO             PET4 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.022 INFO             PET4 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.022 INFO             PET4 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.022 INFO             PET4 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.022 INFO             PET4 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.022 INFO             PET4 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.022 INFO             PET4 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.022 INFO             PET4 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.022 INFO             PET4 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET4 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.022 INFO             PET4 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET4 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.022 INFO             PET4 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.022 INFO             PET4 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.022 INFO             PET4 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.022 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220714 022036.022 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220714 022036.022 INFO             PET4 main: vm located at: 0x1bde080
20220714 022036.022 INFO             PET4 main: petCount=6 localPet=4 mypthid=47435323442432 currentSsiPe=4
20220714 022036.022 INFO             PET4 main: Current system level affinity pinning for local PET:
20220714 022036.022 INFO             PET4 main:  SSIPE=4
20220714 022036.022 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.022 INFO             PET4 main: ssiCount=1 localSsi=0
20220714 022036.022 INFO             PET4 main: mpionly=1 threadsflag=0
20220714 022036.022 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.022 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.022 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.022 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.022 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.022 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET4 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET4 Executing 'userm1_register'
20220714 022036.023 INFO             PET4 Executing 'userm2_setvm'
20220714 022036.023 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.023 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.025 INFO             PET4 Entering 'user1_run'
20220714 022036.025 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET4 model1: vm located at: 0x2c61c20
20220714 022036.025 INFO             PET4 model1: petCount=6 localPet=4 mypthid=47435323442432 currentSsiPe=4
20220714 022036.025 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET4 model1:  SSIPE=4
20220714 022036.025 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET4 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET4 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.025 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022037.523 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022038.940 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022040.357 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022041.774 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022043.191 INFO             PET4 Exiting 'user1_run'
20220714 022050.111 INFO             PET4 Entering 'user1_run'
20220714 022050.111 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220714 022050.111 INFO             PET4 model1: vm located at: 0x2c61c20
20220714 022050.111 INFO             PET4 model1: petCount=6 localPet=4 mypthid=47435323442432 currentSsiPe=4
20220714 022050.111 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220714 022050.111 INFO             PET4 model1:  SSIPE=4
20220714 022050.111 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.111 INFO             PET4 model1: ssiCount=1 localSsi=0
20220714 022050.111 INFO             PET4 model1: mpionly=1 threadsflag=0
20220714 022050.111 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.111 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.111 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.111 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.111 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.111 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.111 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.111 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.111 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220714 022050.111 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022051.528 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022052.946 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022054.362 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022055.780 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220714 022057.197 INFO             PET4 Exiting 'user1_run'
20220714 022104.163 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET4  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET4 Finalizing ESMF
20220714 022036.017 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220714 022036.017 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220714 022036.017 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220714 022036.017 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220714 022036.017 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220714 022036.017 INFO             PET5 Running with ESMF Version   : v8.4.0b04
20220714 022036.017 INFO             PET5 ESMF library build date/time: "Jul 14 2022" "01:53:10"
20220714 022036.017 INFO             PET5 ESMF library build location : /glade/scratch/dunlap/esmf-test-dev/gfortran_10.1.0_mpt_g_develop/esmf
20220714 022036.017 INFO             PET5 ESMF_COMM                   : mpt
20220714 022036.017 INFO             PET5 ESMF_MOAB                   : enabled
20220714 022036.017 INFO             PET5 ESMF_LAPACK                 : enabled
20220714 022036.017 INFO             PET5 ESMF_NETCDF                 : enabled
20220714 022036.017 INFO             PET5 ESMF_PNETCDF                : disabled
20220714 022036.017 INFO             PET5 ESMF_PIO                    : enabled
20220714 022036.017 INFO             PET5 ESMF_YAMLCPP                : enabled
20220714 022036.018 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220714 022036.018 INFO             PET5 esmfComm=mpt
20220714 022036.018 INFO             PET5 isPthreadsEnabled=1
20220714 022036.018 INFO             PET5 isOpenMPEnabled=1
20220714 022036.018 INFO             PET5 isOpenACCEnabled=0
20220714 022036.018 INFO             PET5 isSsiSharedMemoryEnabled=1
20220714 022036.018 INFO             PET5 ssiCount=1 peCount=6
20220714 022036.018 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220714 022036.018 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220714 022036.018 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220714 022036.018 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220714 022036.018 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220714 022036.018 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220714 022036.018 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220714 022036.018 INFO             PET5 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220714 022036.018 INFO             PET5 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220714 022036.018 INFO             PET5 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220714 022036.018 INFO             PET5 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220714 022036.018 INFO             PET5 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220714 022036.018 INFO             PET5 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220714 022036.018 INFO             PET5 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220714 022036.018 INFO             PET5 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220714 022036.018 INFO             PET5 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220714 022036.018 INFO             PET5 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220714 022036.018 INFO             PET5 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220714 022036.018 INFO             PET5 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220714 022036.021 INFO             PET5 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220714 022036.021 INFO             PET5 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220714 022036.021 INFO             PET5 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220714 022036.021 INFO             PET5 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220714 022036.021 INFO             PET5 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220714 022036.021 INFO             PET5 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220714 022036.021 INFO             PET5 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220714 022036.021 INFO             PET5 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220714 022036.021 INFO             PET5 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220714 022036.021 INFO             PET5 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220714 022036.021 INFO             PET5 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220714 022036.021 INFO             PET5 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220714 022036.021 INFO             PET5 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220714 022036.021 INFO             PET5 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220714 022036.021 INFO             PET5 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220714 022036.021 INFO             PET5 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220714 022036.021 INFO             PET5 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220714 022036.021 INFO             PET5 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220714 022036.021 INFO             PET5 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220714 022036.021 INFO             PET5 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220714 022036.021 INFO             PET5 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220714 022036.021 INFO             PET5 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220714 022036.021 INFO             PET5 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220714 022036.021 INFO             PET5 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220714 022036.021 INFO             PET5 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220714 022036.021 INFO             PET5 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220714 022036.021 INFO             PET5 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220714 022036.021 INFO             PET5 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220714 022036.021 INFO             PET5 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220714 022036.021 INFO             PET5 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220714 022036.021 INFO             PET5 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220714 022036.021 INFO             PET5 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220714 022036.021 INFO             PET5 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220714 022036.021 INFO             PET5 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220714 022036.021 INFO             PET5 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220714 022036.021 INFO             PET5 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220714 022036.021 INFO             PET5 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220714 022036.021 INFO             PET5 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220714 022036.021 INFO             PET5 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET5 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220714 022036.021 INFO             PET5 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220714 022036.021 INFO             PET5 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220714 022036.021 INFO             PET5 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220714 022036.021 INFO             PET5 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220714 022036.021 INFO             PET5 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220714 022036.021 INFO             PET5 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220714 022036.021 INFO             PET5 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220714 022036.021 INFO             PET5 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220714 022036.021 INFO             PET5 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220714 022036.021 INFO             PET5 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220714 022036.021 INFO             PET5 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220714 022036.021 INFO             PET5 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220714 022036.021 INFO             PET5 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220714 022036.021 INFO             PET5 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220714 022036.021 INFO             PET5 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220714 022036.021 INFO             PET5 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220714 022036.021 INFO             PET5 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220714 022036.021 INFO             PET5 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220714 022036.021 INFO             PET5 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220714 022036.021 INFO             PET5 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220714 022036.021 INFO             PET5 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220714 022036.021 INFO             PET5 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220714 022036.021 INFO             PET5 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220714 022036.021 INFO             PET5 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220714 022036.021 INFO             PET5 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220714 022036.021 INFO             PET5 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220714 022036.021 INFO             PET5 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220714 022036.021 INFO             PET5 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220714 022036.021 INFO             PET5 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220714 022036.021 INFO             PET5 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220714 022036.021 INFO             PET5 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220714 022036.021 INFO             PET5 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220714 022036.021 INFO             PET5 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220714 022036.021 INFO             PET5 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220714 022036.021 INFO             PET5 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220714 022036.021 INFO             PET5 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220714 022036.021 INFO             PET5 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220714 022036.021 INFO             PET5 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220714 022036.021 INFO             PET5 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220714 022036.021 INFO             PET5 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220714 022036.021 INFO             PET5 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220714 022036.021 INFO             PET5 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220714 022036.021 INFO             PET5 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220714 022036.021 INFO             PET5 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220714 022036.021 INFO             PET5 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220714 022036.021 INFO             PET5 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220714 022036.021 INFO             PET5 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220714 022036.021 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220714 022036.022 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220714 022036.022 INFO             PET5 main: vm located at: 0x1bde120
20220714 022036.022 INFO             PET5 main: petCount=6 localPet=5 mypthid=47435323442432 currentSsiPe=5
20220714 022036.022 INFO             PET5 main: Current system level affinity pinning for local PET:
20220714 022036.022 INFO             PET5 main:  SSIPE=5
20220714 022036.022 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.022 INFO             PET5 main: ssiCount=1 localSsi=0
20220714 022036.022 INFO             PET5 main: mpionly=1 threadsflag=0
20220714 022036.022 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.022 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220714 022036.022 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220714 022036.022 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220714 022036.022 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220714 022036.022 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220714 022036.022 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.022 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220714 022036.022 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220714 022036.023 INFO             PET5 Executing 'userm1_setvm'
20220714 022036.023 INFO             PET5 Executing 'userm1_register'
20220714 022036.023 INFO             PET5 Executing 'userm2_setvm'
20220714 022036.023 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.023 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220714 022036.025 INFO             PET5 Entering 'user1_run'
20220714 022036.025 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220714 022036.025 INFO             PET5 model1: vm located at: 0x2c62cb0
20220714 022036.025 INFO             PET5 model1: petCount=6 localPet=5 mypthid=47435323442432 currentSsiPe=5
20220714 022036.025 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220714 022036.025 INFO             PET5 model1:  SSIPE=5
20220714 022036.025 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022036.025 INFO             PET5 model1: ssiCount=1 localSsi=0
20220714 022036.025 INFO             PET5 model1: mpionly=1 threadsflag=0
20220714 022036.025 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022036.025 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022036.025 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220714 022036.025 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022036.025 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220714 022036.025 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022036.025 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220714 022036.025 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022036.025 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220714 022036.025 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022036.025 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220714 022036.025 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022036.026 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220714 022036.026 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220714 022036.026 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022037.510 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022038.912 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022040.314 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022041.716 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022043.118 INFO             PET5 Exiting 'user1_run'
20220714 022050.111 INFO             PET5 Entering 'user1_run'
20220714 022050.111 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220714 022050.111 INFO             PET5 model1: vm located at: 0x2c62cb0
20220714 022050.111 INFO             PET5 model1: petCount=6 localPet=5 mypthid=47435323442432 currentSsiPe=5
20220714 022050.111 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220714 022050.111 INFO             PET5 model1:  SSIPE=5
20220714 022050.111 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220714 022050.111 INFO             PET5 model1: ssiCount=1 localSsi=0
20220714 022050.111 INFO             PET5 model1: mpionly=1 threadsflag=0
20220714 022050.111 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220714 022050.111 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220714 022050.111 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220714 022050.111 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220714 022050.111 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220714 022050.111 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220714 022050.111 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220714 022050.111 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220714 022050.111 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220714 022050.111 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022051.513 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022052.914 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022054.316 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022055.718 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220714 022057.120 INFO             PET5 Exiting 'user1_run'
20220714 022104.163 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220714 022104.163 INFO             PET5  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220714 022104.163 INFO             PET5 Finalizing ESMF
